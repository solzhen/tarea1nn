{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea1RRNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solzhen/tarea1nn/blob/master/Tarea1RRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E2G0-o7SgrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numbers\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smFSfUu2SmyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActivationFunction:\n",
        "  def apply(x):\n",
        "    '''\n",
        "    Applies function to input X\n",
        "    '''\n",
        "    pass\n",
        "  def derivative(z):\n",
        "    '''\n",
        "    Calculates derivative in terms of the function itself as input z.\n",
        "    Assume z = apply(x)\n",
        "    '''\n",
        "    pass\n",
        "  \n",
        "class Step(ActivationFunction):\n",
        "  def apply(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "  def derivative(z):\n",
        "    return 0\n",
        "  \n",
        "class Sigmoid(ActivationFunction):\n",
        "  def apply(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  def derivative(z):\n",
        "    return z * (1 - z)\n",
        "  \n",
        "class Tanh(ActivationFunction):\n",
        "  def apply(x):\n",
        "    p = np.exp(x)\n",
        "    n = np.exp(-x)\n",
        "    return (p - n) / (p + n)\n",
        "  def derivative(z):\n",
        "    return 1 - np.power(z, 2)\n",
        "  \n",
        "class Relu(ActivationFunction):\n",
        "  def apply(x):\n",
        "    return np.where(x >= 0, x, 0)\n",
        "  def derivative(z):\n",
        "    return np.where(z >= 0, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRH--jEgSspT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neurona:\n",
        "  def __init__(self, pesos, sesgo, funcion):\n",
        "    #print (type(pesos))\n",
        "    if not isinstance(pesos, np.ndarray):\n",
        "      print (\"error1\")\n",
        "      return\n",
        "    if not isinstance(sesgo,numbers.Number):\n",
        "      print (\"error2\")\n",
        "      return\n",
        "    self.pesos = pesos\n",
        "    self.sesgo = sesgo\n",
        "    self.funcion = funcion\n",
        "    self.memory = None\n",
        "  def feed(self, entrada):\n",
        "    return self.funcion.apply(np.dot(entrada, self.pesos) + self.sesgo)\n",
        "  def train(self, entrada, esperado, learn_rate):\n",
        "    real = self.feed(entrada)\n",
        "    error = esperado - real\n",
        "    trans_der = self.funcion.derivative(real)\n",
        "    delta = error * trans_der\n",
        "    self.pesos += entrada * learn_rate\n",
        "    self.sesgo += delta * learn_rate\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW-PQFUvSvFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Red:\n",
        "  def __init__(self, n_inputs, n_outputs, n_layers, n_neurons_layer, \n",
        "               weights = None, biases = None, act_fun = None, learn_r=0.01):\n",
        "    self.n_inputs = n_inputs\n",
        "    self.n_outputs = n_outputs\n",
        "    self.n_layers = n_layers\n",
        "    self.n_neurons_layer = n_neurons_layer\n",
        "    self.learn_r = learn_r\n",
        "    self.grad = []\n",
        "    self.cache = []\n",
        "    \n",
        "    if not weights:\n",
        "      self.weights = []\n",
        "      p = n_inputs\n",
        "      for n_neurons in n_neurons_layer:\n",
        "        self.weights.append( np.random.randn(p,n_neurons) )\n",
        "        p = n_neurons\n",
        "    else:\n",
        "      self.weights = weights\n",
        "      \n",
        "    if not biases:\n",
        "      self.biases = []\n",
        "      for n_neurons in n_neurons_layer:\n",
        "        self.biases.append( np.zeros(n_neurons) )\n",
        "    else:\n",
        "      self.biases = biases\n",
        "      \n",
        "    if not act_fun:\n",
        "      self.act_fun = []\n",
        "      for n_neurons in n_neurons_layer:\n",
        "        self.act_fun.append( [Relu] )\n",
        "    else:\n",
        "      self.act_fun = act_fun  \n",
        "      \n",
        "  def feed(self, x):\n",
        "    self.cache = []\n",
        "    for layer_weights, layer_biases, layer_act_fun in zip(\n",
        "        self.weights, self.biases, self.act_fun):\n",
        "      u = x @ layer_weights + layer_biases\n",
        "      x = layer_act_fun.apply(u)\n",
        "      self.cache.append(x)\n",
        "    return x, self.cache\n",
        "  \n",
        "  def error_propagation(self, init_error):\n",
        "    self.grad = []\n",
        "    for output, layer_act_fun, layer_weights in reversed(\n",
        "        zip(self.cache, self.act_fun, self.weights)):\n",
        "      trans_der = layer_act_fun.derivative(output)\n",
        "      delta = error * trans_der # dim(1,M)\n",
        "      grad.append(delta)      \n",
        "      error = delta @ layer_weights   \n",
        "    self.grad = list(reversed(self.grad))\n",
        "    return self.grad\n",
        "      \n",
        "  def update_parameters(self, inputN):\n",
        "    for i in range(self.n_layers):\n",
        "      self.weights[i] += self.weights[i] + inputN.T * self.learn_r * grad[i]\n",
        "      self.biases[i] += self.biases[i] + self.learn_r * grad[i]\n",
        "      inputN = cache[i]\n",
        "    return self.weights, self.biases\n",
        "      \n",
        "  def train(self, x, y):    \n",
        "    error = y - self.feed(x) # dim(1,N)\n",
        "    self.error_propagation(error)\n",
        "    update_parameters(x)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ahyPW5T-QZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}